{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90ab48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display, Markdown\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, learning_curve\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier, VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, mean_squared_error, RocCurveDisplay, roc_auc_score\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import altair as alt\n",
    "alt.data_transformers.disable_max_rows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9e83a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataframe & variables\n",
    "df = pd.read_csv('./df.csv')\n",
    "categorical_columns = [\n",
    "    'workclass',\n",
    "    'marital-status',\n",
    "    'occupation',\n",
    "    'relationship',\n",
    "    'race',\n",
    "    'sex',\n",
    "    'native-country',\n",
    "    'income'\n",
    " ]\n",
    "numeric_columns = [\n",
    "    'age',\n",
    "    'fnlwgt',\n",
    "    'education-num',\n",
    "    'capital-gain',\n",
    "    'capital-loss',\n",
    "    'hours-per-week'\n",
    " ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2e9796",
   "metadata": {},
   "source": [
    "# Pre-Processing\n",
    "In the Preprocessing stage, I first split the dataset into training and testing sets to ensure that the models can be properly trained and evaluated on separate data. The training set is used to fit and optimize the supervised classification models, while the testing set is reserved for assessing their predictive performance.  \n",
    "\n",
    "Next, I created preprocessing pipelines for both numeric and categorical features. The numeric transformer uses an `SimpleImputer` to fill in missing values based on the medians of the numeric variables, followed by a `StandardScaler` to normalize their ranges. The categorical transformer also uses an `SimpleImputer` with the most frequent category strategy to handle missing values and then applies a `OneHotEncoder` to convert categorical features into numerical format suitable for modeling. These transformations are combined within a `ColumnTransformer` to ensure that each feature type is processed appropriately and consistently, producing a clean and fully prepared dataset for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6926063e",
   "metadata": {},
   "source": [
    "### Train Test Split\n",
    "- The dataset is now divided into a training set that will be used to build supervised classification models, and a testing set for evaluating their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1ea620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the predictors\n",
    "X = df[['age', 'workclass', 'fnlwgt', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain',\n",
    "       'capital-loss', 'hours-per-week', 'native-country']]\n",
    "\n",
    "# Define the target\n",
    "y = df['income']\n",
    "\n",
    "# Drop the target from the categorical_columns list\n",
    "categorical_columns = [col for col in categorical_columns if col != 'income']\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Check shape of X_train, X_test, y_train, & y_test\n",
    "print(f'X_train shape = {X_train.shape}    y_train shape = {y_train.shape}')\n",
    "print(f'X_test shape =  {X_test.shape}     y_test shape =  {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0bf254",
   "metadata": {},
   "source": [
    "### Preprocessing Transformers\n",
    "- Next we build a preprocessing pipeline that prepares numeric and categorical data for modeling. Numeric features are imputed using their median values and standardized for consistent scaling, while categorical features are imputed with the most frequent category and one-hot encoded into binary variables. The `ColumnTransformer` then applies these transformations to their respective columns, producing a clean, model-ready dataset.\n",
    "- To aid in modeling dummy columns will be added during imputing to indicate where data was missing for the `capital-gain`, `capital-loss`, `workclass`, `occupation`, and `native-country` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9322888",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline([\n",
    "    # ('imputer', IterativeImputer(initial_strategy='median', add_indicator=True)),\n",
    "    ('imputer', SimpleImputer(strategy='median', add_indicator=True)),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    # ('imputer', IterativeImputer(initial_strategy='most_frequent', add_indicator=True)),\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent', add_indicator=True)),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_columns),\n",
    "        ('cat', categorical_transformer, categorical_columns)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471c4cb6",
   "metadata": {},
   "source": [
    "### Preprocessing - Conclusions/Discussions/Next Steps:\n",
    "During preprocessing, the main challenge identified was ensuring that the imputation process accurately captured relationships among variables without introducing bias, especially given the number of missing values in both numeric and categorical features.  \n",
    "\n",
    "With the data now standardized and encoded, the next step will be to apply supervised learning models to classify `income` levels and evaluate their performance on the testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e51a365",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "In the Modeling section, I first define several helper functions to streamline the evaluation process by generating learning curves, classification reports, confusion matrices, ROC curves, and feature selection summaries.  \n",
    "\n",
    "Three supervised classification models are developed:\n",
    "- a Logistic Regression model to estimate class probabilities using a sigmoid function,\n",
    "- a Support Vector Classifier (SVC) to separate classes by finding the optimal decision boundary, and\n",
    "- a Random Forest Classifier to aggregate multiple decision trees for improved accuracy and robustness.\n",
    "\n",
    "Each model is trained using the preprocessed training data and evaluated on the test set to measure performance.  \n",
    "\n",
    "Finally, ROC curves are plotted to compare all three classifiers and visually assess their ability to distinguish between `income` classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c3b31f",
   "metadata": {},
   "source": [
    "### Modeling Helper Functions\n",
    "- These helper functions are used to evaluate model performance by creating:\n",
    "  - Learning curve plots (`plot_learning_curve` function)\n",
    "  - Classification tables (`create_classification_output` function)\n",
    "  - Confusion matrices (`create_classification_output` function)\n",
    "  - ROC curves (`make_roc_curves` function)\n",
    "- There are also functions used to:\n",
    "  - Identify dropped columns during feature selection (`show_selected_features` function)\n",
    "  - Extract the best hyperparameters from cross-validation (`best_params_for` function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7883e1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the learning curve for a given model\n",
    "def plot_learning_curve(estimator, X, y, clf_type):\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    train_sizes, train_scores, val_scores = learning_curve(\n",
    "        estimator=estimator,\n",
    "        X=X, y=y,\n",
    "        cv=cv,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1,\n",
    "        train_sizes=np.linspace(0.2, 1.0, 5),  # fewer points = faster\n",
    "        shuffle=True,\n",
    "        random_state=42\n",
    "    )\n",
    "    train_mean, train_std = train_scores.mean(axis=1), train_scores.std(axis=1)\n",
    "    val_mean, val_std = val_scores.mean(axis=1), val_scores.std(axis=1)\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(train_sizes, train_mean, 'o-', label='Training')\n",
    "    plt.plot(train_sizes, val_mean, 'o-', label='Validation')\n",
    "    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1)\n",
    "    plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1)\n",
    "    plt.xlabel('Training set size')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(f'Learning Curve\\n({clf_type})')\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Function to output the learning curve, classification report, and confusion matrix for a given model\n",
    "def create_classification_output(pipe, clf_type):\n",
    "    # Plot learning curve\n",
    "    print('\\n' + '=' * 110)\n",
    "    print(f'{clf_type} Learning Curve:')\n",
    "    plot_learning_curve(pipe, X_train, y_train, clf_type=clf_type)\n",
    "\n",
    "    # Print classification report\n",
    "    print('\\n' + '=' * 110)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    report = classification_report(y_test, y_pred, digits=4)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f'{clf_type} Report:\\n{report}')\n",
    "    print(f'Accuracy: {acc:.4f}')\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    print('\\n' + '=' * 110)\n",
    "    print(f'{clf_type} Confusion Matrix:')\n",
    "    # cls = pipe.named_steps['classifier'].classes_\n",
    "    labels = ['<=50K', '>50K']\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=labels, yticklabels=labels)\n",
    "    plt.title(f'Test Data Confusion Matrix\\n({clf_type})')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return acc\n",
    "\n",
    "# Function to plot ROC curves and a AUC summary chart to compare models\n",
    "def make_roc_curves(X_test, y_test, models):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6), gridspec_kw={'width_ratios': [1, 1]})\n",
    "    aucs, colors = {}, {}\n",
    "\n",
    "    for name, model in models.items():\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            y_score = model.predict_proba(X_test)[:, 1]\n",
    "        else:\n",
    "            y_score = model.decision_function(X_test)\n",
    "\n",
    "        auc = roc_auc_score(y_test, y_score)\n",
    "        aucs[name] = auc\n",
    "        disp = RocCurveDisplay.from_predictions(y_test, y_score, name=name, ax=ax1, pos_label='>50K')\n",
    "        colors[name] = disp.line_.get_color()\n",
    "\n",
    "    # ROC Curves\n",
    "    ax1.plot([0, 1], [0, 1], 'k--', label='Chance')\n",
    "    ax1.set_title('ROC Curves')\n",
    "    ax1.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=1, frameon=False, fontsize=9)\n",
    "\n",
    "    # AUC Sumamry Chart\n",
    "    names, scores = zip(*sorted(aucs.items(), key=lambda kv: kv[1], reverse=True))\n",
    "    bar_colors = [colors[n] for n in names]\n",
    "    bars = ax2.bar(range(len(names)), scores, color=bar_colors)\n",
    "    ax2.set_xticks(range(len(names)))\n",
    "    ax2.set_xticklabels(names, rotation=90, ha='center')\n",
    "    ax2.set_ylabel('AUC'); ax2.set_ylim(0.8, 1.0); ax2.set_title('AUC by Model Type')\n",
    "    for rect, s in zip(bars, scores):\n",
    "        ax2.text(rect.get_x() + rect.get_width()/2.0, rect.get_height() + 0.005, f'{s:.3f}',\n",
    "                 ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "    fig.tight_layout(); fig.subplots_adjust(bottom=0.25)\n",
    "    plt.show()\n",
    "\n",
    "# Function to show which features are being dropped during feature selection\n",
    "def show_selected_features(pipe, clf_type):\n",
    "    preprocess = pipe.named_steps['preprocessor']\n",
    "    selector = pipe.named_steps['selector']\n",
    "\n",
    "    feature_names = preprocess.get_feature_names_out()\n",
    "    kept_mask = selector.get_support()\n",
    "    coef = getattr(selector.estimator_, 'coef_', None)\n",
    "    l1_importance = np.abs(coef).ravel() if coef is not None else np.zeros_like(kept_mask, dtype=float)\n",
    "\n",
    "    selector_df = (\n",
    "        pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'kept': kept_mask,\n",
    "            'l1_importance': l1_importance\n",
    "        })\n",
    "        .assign(status=lambda d: np.where(d.kept, 'kept', 'dropped'))\n",
    "        .sort_values(['kept', 'l1_importance'], ascending=[False, False])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    print('=' * 110)\n",
    "    print(f'{clf_type} Feature Selection:')\n",
    "    print(f'\\n{kept_mask.sum()} of the {kept_mask.size} features are used in modeling\\n')\n",
    "    print('Features dropped from model:')\n",
    "    for feat in selector_df.loc[~selector_df['kept'], 'feature']:\n",
    "        print(f'\\t{feat}')\n",
    "\n",
    "# Function to extract the best model hyperparameters from cross-validation\n",
    "def best_params_for(results, name):\n",
    "    mask = results['param_classifier'].astype(str).str.contains(name)\n",
    "    row = results[mask].sort_values('rank_test_score').iloc[0].dropna()\n",
    "    params = {k: row[k] for k in row.index if k.startswith('param_')}\n",
    "    params = {k.replace('param_', ''): v for k, v in params.items()}\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1485e4d6",
   "metadata": {},
   "source": [
    "### Logistic Classifier\n",
    "- A Logistic Classifier predicts the probability that an observation belongs to a particular class by modeling the relationship between input features and a binary outcome using a logistic (sigmoid) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758c6e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create logistic classifier pipeline instance\n",
    "logistic_pipe = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_transformer),\n",
    "    ('classifier', LogisticRegression(max_iter=2000, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# Fit model\n",
    "logistic_pipe.fit(X_train, y_train)\n",
    "\n",
    "# Create classification output\n",
    "logistic_accuracy = create_classification_output(logistic_pipe, 'Logistic Classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4214e909",
   "metadata": {},
   "source": [
    "> The logistic regression model achieved an overall accuracy of 85.9%, performing strongly in predicting individuals earning ≤50K with higher recall (93.8%) than those earning >50K (60.6%). The learning curve shows stable training and validation accuracy, indicating a well-generalized model with minimal overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cfee68",
   "metadata": {},
   "source": [
    "### Support Vector Classifier\n",
    "- A Support Vector Classifier (SVC) separates classes by finding the optimal hyperplane that maximizes the margin between them, making it effective for both linear and non-linear classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d814969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create support vector classifier pipeline instance\n",
    "svc_pipe = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_transformer),\n",
    "    ('classifier', SVC(kernel='rbf', probability=True))\n",
    "])\n",
    "\n",
    "# Fit model\n",
    "svc_pipe.fit(X_train, y_train)\n",
    "\n",
    "# Create classification output\n",
    "svc_accuracy = create_classification_output(svc_pipe, 'Support Vector Classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13447f2f",
   "metadata": {},
   "source": [
    "> The support vector classifier achieved an overall accuracy of 86.5%, performing very well for predicting ≤50K incomes (95.0% recall) but less effectively for >50K incomes (59.4% recall). The learning curve shows consistent training and validation accuracy, suggesting good generalization with minimal overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc050d0",
   "metadata": {},
   "source": [
    "### Random Forest Classifier\n",
    "- A Random Forest Classifier builds an ensemble of decision trees on random subsets of the data and averages their predictions to improve accuracy and reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d76a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random forest classifier pipeline instance\n",
    "rf_pipe = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_transformer),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=200, random_state=42))\n",
    "])\n",
    "\n",
    "# Fit model\n",
    "rf_pipe.fit(X_train, y_train)\n",
    "\n",
    "# Create classification output\n",
    "rf_accuracy = create_classification_output(rf_pipe, 'Random Forest Classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed0136d",
   "metadata": {},
   "source": [
    "> The random forest classifier achieved an overall accuracy of 85.4%, showing strong performance for predicting ≤50K incomes (92.9% recall) but weaker performance for >50K (61.7% recall). The learning curve indicates near-perfect training accuracy and a noticeable gap with validation accuracy, **suggesting some degree of overfitting**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78971b03",
   "metadata": {},
   "source": [
    "### ROC Curves - Base Classifier Models\n",
    "- An ROC curve (Receiver Operating Characteristic curve) shows how well each classifier distinguishes between the positive and negative classes across different threshold values. It plots the True Positive Rate (sensitivity) against the False Positive Rate (1 - specificity), allowing you to visualize the trade-off between correctly identifying positives and incorrectly classifying negatives. When the logistic, support vector, and random forest classifiers are displayed on the same ROC curve, the one with a line closer to the top-left corner demonstrates better overall performance and a higher ability to separate the two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faadd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_classifier_models = {\n",
    "    'Logistic Regression':  logistic_pipe,\n",
    "    'SVC':                  svc_pipe,\n",
    "    'Random Forest':        rf_pipe,\n",
    "}\n",
    "make_roc_curves(X_test, y_test, base_classifier_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2231a3af",
   "metadata": {},
   "source": [
    "> All three models performed similarly well, with Logistic Regression achieving the highest AUC of 0.908, followed closely by Random Forest (0.903) and SVC (0.899). These results indicate that each model has a strong ability to distinguish between income classes, with only minor differences in overall predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c57300",
   "metadata": {},
   "source": [
    "### Modeling - Conclusions/Discussions/Next Steps:\n",
    "The modeling results showed that all three classifiers performed well, with Logistic Regression achieving the highest AUC (0.908) and accuracy (85.9%), followed closely by SVC and Random Forest. However, each model consistently performed better at predicting ≤50K incomes than >50K, suggesting class imbalance or overlapping feature distributions could limit precision for higher-income predictions.  \n",
    "\n",
    "The next step, Feature Selection, will focus on identifying the most influential variables to simplify the models, improve computational efficiency, and reduce potential overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d7ea62",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "This is the process of identifying and keeping only the most relevant input variables that contribute significantly to a model’s predictions. In supervised classification, this helps improve model performance, reduce overfitting, and make the model more efficient by removing redundant or irrelevant features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581230c2",
   "metadata": {},
   "source": [
    "### Logistic Classifier (with feature selection)\n",
    "- Now we train a logistic regression pipeline that preprocesses the data, performs feature selection using an L1-regularized logistic model to drop less important features, and then fits a logistic regression classifier to the preprocessed and feature selected dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c45264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create logistic classifier pipeline instance with feature selection\n",
    "logistic_fs_pipe = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_transformer),\n",
    "    ('selector', SelectFromModel(LogisticRegression(penalty='l1', solver='liblinear', max_iter=200))),\n",
    "    ('classifier', LogisticRegression(max_iter=2000, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# Fit model\n",
    "logistic_fs_pipe.fit(X_train, y_train)\n",
    "\n",
    "# Show the dropped features\n",
    "show_selected_features(logistic_fs_pipe, 'Logistic Classification (with feature selection)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607ca60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create classification output\n",
    "logistic_fs_accuracy = create_classification_output(logistic_fs_pipe, 'Logistic Classification (with feature selection)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bbdcaf",
   "metadata": {},
   "source": [
    "> After applying feature selection, the logistic regression model achieved an accuracy of 85.8%, maintaining similar performance to the base model while using fewer features. The model continued to predict ≤50K incomes with high recall (93.7%) but showed lower recall (60.7%) for >50K, indicating it generalizes well but may still under-identify higher-income individuals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384d5950",
   "metadata": {},
   "source": [
    "### Support Vector Classifier (with feature selection)\n",
    "- This code builds and trains a support vector classifier pipeline that preprocesses the data, uses an L1-regularized linear SVC to select the most important features, and then fits an RBF-kernel SVC for final classification.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c10a50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create support vector classifier pipeline instance\n",
    "svc_fs_pipe = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_transformer),\n",
    "    ('selector', SelectFromModel(LinearSVC(penalty=\"l1\", dual=False, C=0.25, tol=1e-3, max_iter=50000))),\n",
    "    ('classifier', SVC(kernel='rbf', probability=True, random_state=42))\n",
    "])\n",
    "\n",
    "# Fit model\n",
    "svc_fs_pipe.fit(X_train, y_train)\n",
    "\n",
    "# Show the dropped features\n",
    "show_selected_features(svc_fs_pipe, 'Support Vector Classifier (with feature selection)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dc4d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create classification output\n",
    "svc_fs_accuracy = create_classification_output(svc_fs_pipe, 'Support Vector Classifier (with feature selection)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e301f33",
   "metadata": {},
   "source": [
    "> With feature selection applied, the support vector classifier achieved an accuracy of 86.4%, maintaining strong generalization and consistent performance with the base model. It performed very well for predicting ≤50K incomes (95.0% recall) but continued to show lower recall (58.9%) for >50K, indicating slightly better precision but limited improvement in identifying higher-income individuals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4727564d",
   "metadata": {},
   "source": [
    "### Random Forest Classifier (with feature selection)\n",
    "- This code creates and trains a random forest pipeline that preprocesses the data, selects important features based on feature importance scores from an initial random forest, and then fits a final random forest classifier using those selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37853ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random forest classifier pipeline instance\n",
    "rf_fs_pipe = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_transformer),\n",
    "    ('selector', SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1))),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# Fit model\n",
    "rf_fs_pipe.fit(X_train, y_train)\n",
    "\n",
    "# Show the dropped features\n",
    "show_selected_features(rf_fs_pipe, 'Random Forest Classifier (with feature selection)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0f964d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create classification output\n",
    "rf_fs_accuracy = create_classification_output(rf_fs_pipe, 'Random Forest Classifier (with feature selection)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d6b203",
   "metadata": {},
   "source": [
    "> After applying feature selection, the random forest classifier achieved an accuracy of 84.4%, performing well on ≤50K incomes (91.9% recall) but less effectively on >50K (60.6% recall). The learning curve shows near-perfect training accuracy and lower validation accuracy, indicating persistent overfitting despite feature reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e59699b",
   "metadata": {},
   "source": [
    "### ROC Curves - Base vs. Feature Selected Models\n",
    "- Comparing the base classifiers to the classifiers that have had feature selection applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf13ad54",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_and_feature_selection_models = {\n",
    "    'Logistic Regression (base model)':             logistic_pipe,\n",
    "    'SVC (base model)':                             svc_pipe,\n",
    "    'Random Forest (base model)':                   rf_pipe,\n",
    "    'Logistic Regression (with feature selection)': logistic_fs_pipe,\n",
    "    'SVC (with feature selection)':                 svc_fs_pipe,\n",
    "    'Random Forest (with feature selection)':       rf_fs_pipe,\n",
    "}\n",
    "\n",
    "make_roc_curves(X_test, y_test, base_and_feature_selection_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e9e389",
   "metadata": {},
   "source": [
    "> The ROC AUC results show that feature selection had minimal impact on overall model performance, with all models achieving similar AUC scores between 0.89 and 0.91. Logistic Regression maintained the highest AUC at 0.91 both before and after feature selection, indicating strong and consistent classification performance across all configurations.  \n",
    "> \n",
    "> The SVC and Random Forest models with feature selection performed slightly worse because the feature selection process likely removed variables that, while individually weak, collectively contributed useful information to the models’ predictive power. Random Forests, in particular, are robust to irrelevant features and can internally handle feature importance, so external selection can actually reduce their effectiveness. Similarly, the SVC with a non-linear RBF kernel captures complex interactions between features, and removing some of them can disrupt those relationships, leading to a small drop in performance. In short, the feature selection simplified the models too much, slightly reducing their ability to capture subtle patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f215ff",
   "metadata": {},
   "source": [
    "### Feature Selection - Conclusions/Discussions/Next Steps:\n",
    "The feature selection results showed that reducing the number of features had little impact on overall model performance, with all models maintaining similar AUC scores between 0.89 and 0.91. Logistic Regression remained the strongest performer, while the SVC and Random Forest models showed slight declines, likely because the feature selection process removed variables that contributed subtle but meaningful information.  \n",
    "\n",
    "The next step, Hyperparameter Tuning, will focus on optimizing each model’s parameters using cross-validation to improve predictive accuracy and further refine model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00879e9c",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "- This is the process of finding the best combination of model settings (such as regularization strength or tree depth) that optimize performance on unseen data.\n",
    "- Using 5-fold cross-validation, the training data is split into five parts—four folds are used to train the model and one is used to validate it, repeating this process five times so each fold serves as validation once.\n",
    "- The average performance across all folds helps identify the hyperparameters that generalize best to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848eec4b",
   "metadata": {},
   "source": [
    "### Cross Validation\n",
    "- This code performs hyperparameter tuning using GridSearchCV with 5-fold stratified cross-validation to find the best model configurations for logistic regression, SVC, and random forest classifiers—both with and without feature selection. It systematically tests combinations of hyperparameters (like C, max_depth, and selection thresholds) to identify the setup that achieves the highest accuracy across the folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae257102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a default pipeline to be used for cross-validation\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor_transformer),\n",
    "    ('selector', 'passthrough'),   # default; swapped per grid\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Logistic Regression (with no feature selection)\n",
    "logistic_without_feature_selection = {\n",
    "    'classifier': [LogisticRegression(max_iter=2000, n_jobs=-1)],\n",
    "    'classifier__C': [0.01, 0.1, 1, 10],\n",
    "    'selector': ['passthrough']  # no selector__* keys here\n",
    "}\n",
    "\n",
    "# Logistic Regression (with feature selection)\n",
    "logistic_with_feature_selection = {\n",
    "    'classifier': [LogisticRegression(max_iter=2000, n_jobs=-1)],\n",
    "    'classifier__C': [0.01, 0.1, 1, 10],\n",
    "    'selector': [SelectFromModel(\n",
    "        LogisticRegression(penalty='l1', solver='liblinear', max_iter=2000)\n",
    "    )],\n",
    "    'selector__threshold': ['median', '1.5*mean', None]\n",
    "}\n",
    "\n",
    "# Support Vector Classifier (with no feature selection)\n",
    "svc_without_feature_selection = {\n",
    "    'classifier': [SVC(probability=True)],\n",
    "    'classifier__C': [0.5, 1.0, 1.5, 2.0],\n",
    "    'selector': ['passthrough']\n",
    "}\n",
    "\n",
    "# Support Vector Classifier (with feature selection)\n",
    "svc_with_feature_selection = {\n",
    "    'classifier': [SVC(probability=True)],\n",
    "    'classifier__C': [0.5, 1.0, 1.5, 2.0],\n",
    "    'selector': [SelectFromModel(\n",
    "        LinearSVC(penalty='l1', dual=False, max_iter=50000, tol=1e-3)\n",
    "    )],\n",
    "    'selector__threshold': ['median', None]\n",
    "}\n",
    "\n",
    "# Random Forest Classifier (with no feature selection)\n",
    "rf_without_feature_selection = {\n",
    "    'classifier': [RandomForestClassifier(random_state=42, n_jobs=-1)],\n",
    "    'classifier__max_depth': [1, 5, 10],\n",
    "    'classifier__min_samples_leaf': [1, 3, 5],\n",
    "    'selector': ['passthrough']\n",
    "}\n",
    "\n",
    "# Random Forest Classifier (with feature selection)\n",
    "rf_with_feature_selection = {\n",
    "    'classifier': [RandomForestClassifier(random_state=42, n_jobs=-1)],\n",
    "    'classifier__max_depth': [1, 5, 10],\n",
    "    'classifier__min_samples_leaf': [1, 3, 5],\n",
    "    'selector': [SelectFromModel(RandomForestClassifier(\n",
    "        n_estimators=200, random_state=42, n_jobs=-1\n",
    "    ))],\n",
    "    'selector__threshold': ['median', None]\n",
    "}\n",
    "\n",
    "# Define the parameter grid used for cross-validation\n",
    "param_grid = [\n",
    "    logistic_without_feature_selection, logistic_with_feature_selection,\n",
    "    svc_without_feature_selection,      svc_with_feature_selection,\n",
    "    rf_without_feature_selection,       rf_with_feature_selection\n",
    "]\n",
    "\n",
    "# Create a grid search instance\n",
    "gs = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit the grid search model to the training data\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "# Display a dataframe of the cross-validation results\n",
    "cv_results = pd.DataFrame(gs.cv_results_).sort_values('rank_test_score')\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4aa253",
   "metadata": {},
   "source": [
    "### Best Cross-Validated Model\n",
    "- This is the best model and associated hyperparameters that achieve the highest value of accuracy of all models tested during cross-validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bef492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the best cross-validated model hyperparameters\n",
    "print('Best cross-validated model & hyperparameters:')\n",
    "for k,v in gs.best_params_.items():\n",
    "    v=None if v=='passthrough' else v\n",
    "    print(f'\\t{k}:\\t{v}')\n",
    "print(f'Best cross-validated accuracy for the above model = {gs.best_score_:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5968b73d",
   "metadata": {},
   "source": [
    "> After cross-validation the best model is a **support vector classifier with no feature selection applied and a C-value=`1.5`**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05aa3723",
   "metadata": {},
   "source": [
    "### Best Cross-Validated Hyperparameters for Each Classifier\n",
    "- This displays the best hyperparameters that were found during cross-validation for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e8ff7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best GridSearchCV parameters for LogisticRegression\n",
    "logistic_best_cv_params = best_params_for(cv_results, 'LogisticRegression')\n",
    "print('Best cross-validated hyperparameters for Logistic Classifier:')\n",
    "for k,v in logistic_best_cv_params.items():\n",
    "    v=None if v=='passthrough' else v\n",
    "    print(f'\\t{k}:\\t{v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f12b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best GridSearchCV parameters for SVC\n",
    "svc_best_cv_params = best_params_for(cv_results, 'SVC')\n",
    "print('Best cross-validated hyperparameters for Support Vector Classifier:')\n",
    "for k,v in svc_best_cv_params.items():\n",
    "    v=None if v=='passthrough' else v\n",
    "    print(f'\\t{k}:\\t{v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62973c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best GridSearchCV parameters for RandomForestClassifier\n",
    "rf_best_cv_params = best_params_for(cv_results, 'RandomForestClass')\n",
    "print('Best cross-validated hyperparameters for Random Forest Classifier:')\n",
    "for k,v in rf_best_cv_params.items():\n",
    "    v=None if v=='passthrough' else v\n",
    "    print(f'\\t{k}:\\t{v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124b8489",
   "metadata": {},
   "source": [
    "> The best models, as determined by cross validation, for the Logistic Classifier and Support Vector Classifier **do not use feature selection** (as indicated by \"selector:None\" above); whereas the best Random Forest Classifier **does use feature selection**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821bcd57",
   "metadata": {},
   "source": [
    "### Logistic Classifier (hyperparameter tuned)\n",
    "- Applying the best model parameters as found by cross validation for the Logistic Classifier (i.e. `logistic_best_cv_params`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ff150b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build pipeline with the best LogisticRegression hyperparameters\n",
    "logistic_tuned_pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor_transformer),\n",
    "    ('selector', 'passthrough'), # default; swapped if the tuned hyperparameters use feature selection\n",
    "    ('classifier', LogisticRegression(max_iter=2000, n_jobs=-1))\n",
    "]).set_params(**{k: v for k, v in logistic_best_cv_params.items() if k != 'classifier'})\n",
    "\n",
    "# Fit model\n",
    "logistic_tuned_pipe.fit(X_train, y_train)\n",
    "\n",
    "# Create classification output\n",
    "logistic_tuned_accuracy = create_classification_output(logistic_tuned_pipe, 'Logistic Classification (hyperparameter tuned)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a450e4",
   "metadata": {},
   "source": [
    "> After hyperparameter tuning, the logistic regression model achieved an accuracy of 85.9%, showing balanced training and validation performance with minimal overfitting. The optimized parameters maintained strong results for predicting ≤50K incomes (93.8% recall) but continued to show lower recall (60.6%) for >50K, indicating consistent performance with slightly improved model stability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34b3b84",
   "metadata": {},
   "source": [
    "### Support Vector Classifier (hyperparameter tuned)\n",
    "- Applying the best model parameters as found by cross validation for the Support Vector Classifier (i.e. `svc_best_cv_params`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae884c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build pipeline with the best SVC hyperparameters\n",
    "svc_tuned_pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor_transformer),\n",
    "    ('selector', 'passthrough'), # default; swapped if the tuned hyperparameters use feature selection\n",
    "    ('classifier', SVC(kernel='rbf', C=2.0, probability=True))\n",
    "]).set_params(**{k: v for k, v in svc_best_cv_params.items() if k != 'classifier'})\n",
    "\n",
    "# Fit model\n",
    "svc_tuned_pipe.fit(X_train, y_train)\n",
    "\n",
    "# Create classification output\n",
    "svc_tuned_accuracy = create_classification_output(svc_tuned_pipe, 'Support Vector Classifier (hyperparameter tuned)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c929c3fe",
   "metadata": {},
   "source": [
    "> After hyperparameter tuning, the support vector classifier achieved an accuracy of 86.5%, showing slightly improved validation performance and balanced generalization. The model performed exceptionally well for predicting ≤50K incomes (94.9% recall) but continued to struggle with >50K predictions (59.9% recall), indicating a persistent challenge with class imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600c05fb",
   "metadata": {},
   "source": [
    "### Random Forest Classifier (hyperparameter tuned)\n",
    "- Applying the best model parameters as found by cross validation for the Random Forest Classifier (i.e. `rf_best_cv_params`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9eeea88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build pipeline with the best RandomForestClassifier hyperparameters\n",
    "rf_tuned_pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor_transformer),\n",
    "    ('selector', 'passthrough'), # default; swapped if the tuned hyperparameters use feature selection\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "]).set_params(**{k:int(v) if isinstance(v, np.float64) else v for k,v in rf_best_cv_params.items() if k != 'classifier'})\n",
    "\n",
    "# Fit model\n",
    "rf_tuned_pipe.fit(X_train, y_train)\n",
    "\n",
    "# Create classification output\n",
    "rf_tuned_accuracy = create_classification_output(rf_tuned_pipe, 'Random Forest Classifier (hyperparameter tuned)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90bd7fa",
   "metadata": {},
   "source": [
    "> After hyperparameter tuning, the random forest classifier achieved an accuracy of 85.9%, showing improved balance between training and validation accuracy. The model performed very well for ≤50K incomes (95.6% recall) but continued to struggle with >50K predictions (54.9% recall), suggesting that while tuning improved generalization, class imbalance remains a limiting factor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0f70ae",
   "metadata": {},
   "source": [
    "### ROC Curves - Base vs. Hyperparameter Tuned Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012f2c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_and_tuned_models = {\n",
    "    'Logistic Regression (base model)':     logistic_pipe,\n",
    "    'SVC (base model)':                     svc_pipe,\n",
    "    'Random Forest (base model)':           rf_pipe,\n",
    "    'Logistic Regression (tuned model)':    logistic_tuned_pipe,\n",
    "    'SVC (tuned model)':                    svc_tuned_pipe,\n",
    "    'Random Forest (tuned model)':          rf_tuned_pipe,\n",
    "}\n",
    "\n",
    "make_roc_curves(X_test, y_test, base_and_tuned_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc35b03",
   "metadata": {},
   "source": [
    "> The ROC AUC results show that hyperparameter optimization produced only minor performance improvements, with all models achieving similar AUC scores between 0.89 and 0.91. The Random Forest (tuned) model performed best overall (AUC = 0.911), closely followed by both Logistic Regression versions (AUC = 0.908–0.91), indicating that tuning helped refine the models but did not significantly alter their relative performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7aabb41",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning - Conclusions/Discussions/Next Steps:\n",
    "The hyperparameter tuning results showed that optimization slightly improved model stability and generalization, with all models achieving similar AUC scores between 0.89 and 0.91. The Random Forest model performed best overall after tuning, though all classifiers continued to struggle with accurately predicting >50K incomes due to class imbalance.  \n",
    "\n",
    "The next step, Ensemble Modeling, will combine multiple tuned classifiers using voting and stacking techniques to leverage their individual strengths and improve overall predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f174b0",
   "metadata": {},
   "source": [
    "# Ensemble Methods\n",
    "Ensemble methods like `VotingClassifier` and `StackingClassifier` combine multiple machine learning models to improve overall predictive performance. The `VotingClassifier` aggregates predictions from several base models and makes a final decision based on majority vote (for classification) or average (for regression). The `StackingClassifier`, on the other hand, uses the outputs of multiple base models as inputs to a “meta-model,” which learns how to best combine their predictions for improved accuracy and generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8efe32",
   "metadata": {},
   "source": [
    "### Voting Classifier\n",
    "- This code creates and trains a soft voting ensemble that combines the tuned logistic regression, SVC, and random forest models, averaging their predicted probabilities to produce a final, more robust classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fe921f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a VotingClassifier ensemble instance\n",
    "voter = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('lr', logistic_tuned_pipe),\n",
    "        ('svc', svc_tuned_pipe),\n",
    "        ('rf',  rf_tuned_pipe)\n",
    "    ],\n",
    "    voting='soft',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the VotingClassifier ensemble model\n",
    "voter.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21163399",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create classification output\n",
    "voting_accuracy = create_classification_output(voter, 'Voting Classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee51f7e",
   "metadata": {},
   "source": [
    "> fdsd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b460b07",
   "metadata": {},
   "source": [
    "### Stacking Classifier\n",
    "- This code builds and trains a stacking ensemble that combines predictions from the tuned logistic regression, SVC, and random forest models, using a logistic regression meta-model to learn the best way to blend their outputs for improved overall accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195efca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StackingClassifier ensemble instance\n",
    "stacker = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('lr',  logistic_tuned_pipe),\n",
    "        ('svc', svc_tuned_pipe),\n",
    "        ('rf',  rf_tuned_pipe)\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(max_iter=2000),\n",
    "    passthrough=False,\n",
    "    cv=cv,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the StackingClassifier ensemble model\n",
    "stacker.fit(X_train, y_train)\n",
    "\n",
    "# Create classification output\n",
    "stacker_accuracy = create_classification_output(stacker, 'Stacking Classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2565b0a3",
   "metadata": {},
   "source": [
    "> fadj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fbaadd",
   "metadata": {},
   "source": [
    "### ROC Curves - Base vs. Ensemble Models\n",
    "- Here we are comparing the performance of the base models and ensemble models by plotting their ROC curves on the test data to visually evaluate which approach best distinguishes between the income classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaa46a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_and_ensemble_models = {\n",
    "    'Logistic Regression (base model)': logistic_pipe,\n",
    "    'SVC (base model)':                 svc_pipe, \n",
    "    'Random Forest (base model)':       rf_pipe,\n",
    "    'Voting Ensemble':                  voter,\n",
    "    'Stacking Ensemble':                stacker, \n",
    "}\n",
    "\n",
    "make_roc_curves(X_test, y_test, base_and_ensemble_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e079000c",
   "metadata": {},
   "source": [
    "> dsaj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7277df18",
   "metadata": {},
   "source": [
    "### Conclusions/Discussions/Next Steps:\n",
    "1. Summarize the steps taken to clean the dataset\n",
    "2. Identify any insights/findings made while cleaning (including any foreseen difficulties that could occur during analysis)\n",
    "3. Give a brief description of what the next step will be in the analysis (Modeling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef36dd97",
   "metadata": {},
   "source": [
    "# Results and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fc3ee5",
   "metadata": {},
   "source": [
    "- A summary of results and analysis which includes:\n",
    "  1. Proper visualizations (E.g., tables, graphs/plots, heat maps, statistics summary with interpretation, etc.)\n",
    "  2. Use various evaluation metrics (E.g., if your data is imbalanced, there are other metrics (F1, ROC, or AUC) that are better than mere accuracy).\n",
    "     1. Explain why they chose the metric?\n",
    "  3. Iterate the training and evaluation process and improve the performance\n",
    "     1.  Address selecting features through the iteration process\n",
    "  4. Compare the results from the multiple models and make appropriate comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615fb800",
   "metadata": {},
   "source": [
    "### ROC Curves - All Models\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161075b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_and_ensemble_models = {\n",
    "    'Logistic Regression (base model)':             logistic_pipe,\n",
    "    'SVC (base model)':                             svc_pipe, \n",
    "    'Random Forest (base model)':                   rf_pipe,\n",
    "    'Logistic Regression (with feature selection)': logistic_fs_pipe,\n",
    "    'SVC (with feature selection)':                 svc_fs_pipe,\n",
    "    'Random Forest (with feature selection)':       rf_fs_pipe,\n",
    "    'Logistic Regression (tuned model)':            logistic_tuned_pipe,\n",
    "    'SVC (tuned model)':                            svc_tuned_pipe,\n",
    "    'Random Forest (tuned model)':                  rf_tuned_pipe,\n",
    "    'Voting Ensemble':                              voter,\n",
    "    'Stacking Ensemble':                            stacker,\n",
    "}\n",
    "\n",
    "make_roc_curves(X_test, y_test, all_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1001bd31",
   "metadata": {},
   "source": [
    "### Conclusions/Discussions/Next Steps:\n",
    "1. Summarize the steps taken to clean the dataset\n",
    "2. Identify any insights/findings made while cleaning (including any foreseen difficulties that could occur during analysis)\n",
    "3. Give a brief description of what the next step will be in the analysis (Modeling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203e83c2",
   "metadata": {},
   "source": [
    "# Discussion & Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3668f88",
   "metadata": {},
   "source": [
    "1. Learning and takeaways\n",
    "2. Why something didn’t work\n",
    "3. Suggest ways to improve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ddcaa2",
   "metadata": {},
   "source": [
    "Modeling improvements:\n",
    "- Change OneHotEncode education instead of using education-num ordinal encoding\n",
    "- Leave numeric zeros in capital-gain and capital-loss\n",
    "- Typically for categorical columns I would use sklearn's `SimpleImputer` and replace missing values with the most frequent category for the `workclass` and `occupation` columns. But because the missingness appears to be consistent between the two columns the most frequent imputation methods would create an artificial correlation structure between the columns.\n",
    "- In this case sklearn's `IterativeImputer` should be used instead. It will model each feature as a function of the others, so imputations are more coherent across columns. It is slower than `SimpleImputer` but imputes more realistic values.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ff4b51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
